{
  "cells": [
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2025-02-21T15:24:18.000739Z"
        }
      },
      "source": [
        "import torchvision\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Create a vector of zeros of size 5\n",
        "size = (128, 128)\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.Resize(size), torchvision.transforms.ToTensor()])\n",
        "train_dataset = list(torchvision.datasets.Flowers102('./flowers', 'train', transform=transform, download=True))\n",
        "test_dataset = list(torchvision.datasets.Flowers102('./flowers', 'test', transform=transform, download=True))"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "\n",
        "class MyModel(torch.nn.Module):\n",
        "  def __init__(self, layer_size = [512, 512, 512]):\n",
        "    super(MyModel, self).__init__()\n",
        "    layers = []\n",
        "    layers.append(torch.nn.Flatten())\n",
        "    c = 128*128*3\n",
        "    for s in layer_size:\n",
        "      layers.append(torch.nn.Linear(c, s))\n",
        "      layers.append(torch.nn.ReLU())\n",
        "      c = s\n",
        "    layers.append(torch.nn.Linear(c, 102))\n",
        "    self.model = torch.nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x) -> Any:\n",
        "    return self.model(x)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "model = MyModel(layer_size=[])\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "for epoch in range(10):\n",
        "  for imgs, labels in train_loader:\n",
        "    if torch.cuda.is_available():\n",
        "      imgs, labels = imgs.cuda(), labels.cuda()\n",
        "\n",
        "    pred = model(imgs)\n",
        "\n",
        "    loss_val = loss_fn(pred, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss_val.backward()\n",
        "    optimizer.step()\n",
        "    print(loss_val.item())"
      ],
      "metadata": {
        "id": "FI8rRIBzJRjW",
        "outputId": "19434214-3bbc-4514-b4ef-99fa6a1d459a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FI8rRIBzJRjW",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.6438212394714355\n",
            "4.63869047164917\n",
            "4.622243881225586\n",
            "4.696906089782715\n",
            "4.669137954711914\n",
            "4.733333587646484\n",
            "4.695249080657959\n",
            "4.610601902008057\n",
            "4.680362224578857\n",
            "4.658329010009766\n",
            "4.580753803253174\n",
            "4.644181728363037\n",
            "4.645013332366943\n",
            "4.652316093444824\n",
            "4.657857894897461\n",
            "4.684852123260498\n",
            "4.583907604217529\n",
            "4.597551345825195\n",
            "4.548392295837402\n",
            "4.563332557678223\n",
            "4.588440418243408\n",
            "4.542766094207764\n",
            "4.5587849617004395\n",
            "4.5412492752075195\n",
            "4.547906875610352\n",
            "4.558681011199951\n",
            "4.597301483154297\n",
            "4.569085121154785\n",
            "4.572646141052246\n",
            "4.569161415100098\n",
            "4.573310375213623\n",
            "4.559480667114258\n",
            "4.453690052032471\n",
            "4.478829860687256\n",
            "4.479855060577393\n",
            "4.497714996337891\n",
            "4.477872848510742\n",
            "4.52337646484375\n",
            "4.424144744873047\n",
            "4.488255977630615\n",
            "4.449944972991943\n",
            "4.539926052093506\n",
            "4.455214023590088\n",
            "4.505541801452637\n",
            "4.5059404373168945\n",
            "4.474326133728027\n",
            "4.4668450355529785\n",
            "4.492645740509033\n",
            "4.37955379486084\n",
            "4.42859411239624\n",
            "4.351313591003418\n",
            "4.372839450836182\n",
            "4.420522212982178\n",
            "4.442741870880127\n",
            "4.365894317626953\n",
            "4.437900543212891\n",
            "4.358684062957764\n",
            "4.407472133636475\n",
            "4.439809322357178\n",
            "4.397574424743652\n",
            "4.370152473449707\n",
            "4.398980140686035\n",
            "4.424958229064941\n",
            "4.477487087249756\n",
            "4.319134712219238\n",
            "4.317605495452881\n",
            "4.315497875213623\n",
            "4.224481582641602\n",
            "4.290847301483154\n",
            "4.390378952026367\n",
            "4.3356547355651855\n",
            "4.298732280731201\n",
            "4.4111480712890625\n",
            "4.319809913635254\n",
            "4.352459907531738\n",
            "4.366589069366455\n",
            "4.285972595214844\n",
            "4.334005355834961\n",
            "4.342526435852051\n",
            "4.337394714355469\n",
            "4.2585859298706055\n",
            "4.251450061798096\n",
            "4.237544059753418\n",
            "4.247308731079102\n",
            "4.138856410980225\n",
            "4.279306411743164\n",
            "4.270637035369873\n",
            "4.203092575073242\n",
            "4.298370838165283\n",
            "4.277261257171631\n",
            "4.2703962326049805\n",
            "4.256039619445801\n",
            "4.277700424194336\n",
            "4.291399002075195\n",
            "4.2161478996276855\n",
            "4.2834577560424805\n",
            "4.21991491317749\n",
            "4.189542770385742\n",
            "4.132134437561035\n",
            "4.158910751342773\n",
            "4.167133331298828\n",
            "4.202759742736816\n",
            "4.215270519256592\n",
            "4.172098159790039\n",
            "4.168971061706543\n",
            "4.208874225616455\n",
            "4.113890647888184\n",
            "4.265350341796875\n",
            "4.169526100158691\n",
            "4.129991054534912\n",
            "4.244691371917725\n",
            "4.162780284881592\n",
            "4.101686954498291\n",
            "4.1146955490112305\n",
            "4.057987689971924\n",
            "4.110611915588379\n",
            "4.169332981109619\n",
            "4.177627086639404\n",
            "4.182754039764404\n",
            "4.095301628112793\n",
            "4.098978042602539\n",
            "4.061128616333008\n",
            "4.009687900543213\n",
            "4.110406398773193\n",
            "4.119998931884766\n",
            "4.181297779083252\n",
            "4.096597194671631\n",
            "4.06805944442749\n",
            "4.0216875076293945\n",
            "4.0066704750061035\n",
            "4.051018238067627\n",
            "4.112684726715088\n",
            "4.040157318115234\n",
            "4.003312110900879\n",
            "4.070896148681641\n",
            "4.073276042938232\n",
            "4.0321364402771\n",
            "4.0784912109375\n",
            "4.111164093017578\n",
            "3.955756664276123\n",
            "3.9709622859954834\n",
            "4.060854911804199\n",
            "4.021895885467529\n",
            "4.118603706359863\n",
            "3.9103379249572754\n",
            "3.9680116176605225\n",
            "3.9167544841766357\n",
            "3.9315037727355957\n",
            "3.9602761268615723\n",
            "4.039480209350586\n",
            "4.050904750823975\n",
            "4.053009986877441\n",
            "3.905508279800415\n",
            "3.974618673324585\n",
            "4.038945198059082\n",
            "3.9591658115386963\n",
            "3.9531633853912354\n",
            "3.9877560138702393\n",
            "4.119935035705566\n",
            "3.9278533458709717\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}